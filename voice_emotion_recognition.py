# -*- coding: utf-8 -*-
"""Voice_Emotion_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wn2iCk_PjaLu6Y0y5uRpqVjhrtBXrTT6

**Mount Drive**
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Install Libraries**"""

!pip install librosa soundfile resampy gradio

"""**Import Libraries**"""

import os
import librosa
import numpy as np
import pandas as pd
import gradio as gr

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

import seaborn as sns
import matplotlib.pyplot as plt
import joblib

"""**Load Dataset**"""

dataset_path = '/content/drive/MyDrive/dataset/ravdess'

"""**Feature Extraction Function**"""

def extract_features(file_path):
    y, sr = librosa.load(file_path, duration=3, offset=0.5)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr)

    mfccs = np.mean(mfcc.T, axis=0)
    chroma_mean = np.mean(chroma.T, axis=0)
    mel_mean = np.mean(mel.T, axis=0)

    return np.hstack([mfccs, chroma_mean, mel_mean])

"""**Extract Feature**"""

features = []
labels = []

emotion_map = {
    "01": "neutral", "02": "calm", "03": "happy", "04": "sad",
    "05": "angry", "06": "fearful", "07": "disgust", "08": "surprised"
}

for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if file.endswith(".wav"):
            file_path = os.path.join(root, file)
            emotion_code = file.split("-")[2]
            emotion = emotion_map.get(emotion_code)
            if emotion:
                features.append(extract_features(file_path))
                labels.append(emotion)

"""**Preprocess Data (Label Encoding + Scaling)**"""

X = np.array(features)
y = np.array(labels)

le = LabelEncoder()
y_encoded = le.fit_transform(y)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""**Train/Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

"""**Train Model (Random Forest Classifier)**"""

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

"""**Evaluate Model**"""

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

"""**Plot Confusion Matrix**"""

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=le.classes_, yticklabels=le.classes_, cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

""" **Save Model and Preprocessors**"""

joblib.dump(clf, "emotion_model.pkl")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(le, "label_encoder.pkl")

"""**Gradio Interface**"""

import gradio as gr
import librosa
import numpy as np
import joblib

# Extract Features Function
def extract_features_for_gradio(file_path):
    y, sr = librosa.load(file_path, duration=3, offset=0.5)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    mel = librosa.feature.melspectrogram(y=y, sr=sr)

    mfccs = np.mean(mfcc.T, axis=0)
    chroma_mean = np.mean(chroma.T, axis=0)
    mel_mean = np.mean(mel.T, axis=0)

    return np.hstack([mfccs, chroma_mean, mel_mean])

#Load Model & Preprocessors
clf = joblib.load("emotion_model.pkl")
scaler = joblib.load("scaler.pkl")
le = joblib.load("label_encoder.pkl")

#Prediction Function
def predict_emotion_from_audio(file):
    try:
        features = extract_features_for_gradio(file)
        features_scaled = scaler.transform([features])
        prediction = clf.predict(features_scaled)[0]
        emotion = le.inverse_transform([prediction])[0]
        return f"Detected Emotion: {emotion.capitalize()}"
    except Exception as e:
        return f"Error: {e}"

#Welcome message
welcome_message = "**Welcome!**\n\nPlease upload a `.wav` audio file (3–4 seconds) and I'll tell you the speaker's emotion."

#Gradio Interface
interface = gr.Interface(
    fn=predict_emotion_from_audio,
    inputs=gr.Audio(sources=["upload"], type="filepath", label="Upload a .wav audio file"),
    outputs=gr.Text(label="Predicted Emotion"),
    title="Voice Emotion Recognition",
    description="This AI model predicts the **emotion** of a speaker based on their voice.\n\n Upload a `.wav` audio file (3–4 seconds) and see the predicted emotion",
    theme="default",
    examples=None,
    live=False,
)

# Set welcome message as default output
interface.output_component.update(value=welcome_message)

# Launch App
interface.launch(debug=True)